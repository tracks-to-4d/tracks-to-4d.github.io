<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0042)https://research.nvidia.com/labs/par/calm/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252"><script src="" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 150px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}

.kitti {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}

.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}



</style>


</head><body><div class="topnav" id="myTopnav">
  <a href="https://www.nvidia.com/"><img width="100%" src="./files/nvidia.svg"></a>
  <a href="https://www.nvidia.com/en-us/research/"><strong>NVIDIA Research</strong></a>
</div>

<!-- End : Google Analytics Code -->
<script type="text/javascript" src="./files/style/hidebib.js"></script>
<link href="./files/style/css" rel="stylesheet" type="text/css">

    <title>Learning Priors for Non-Rigid SfM from Casual Videos    </title>
    <meta property="og:description" content="Learning Priors for Non-Rigid SfM from Casual Videos    ">
    <link href="./files/style/css2" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async="" src="./files/style/js"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G');
</script>




 
<div class="container">
    <div class="paper-title">
      <h1>Learning Priors for Non-Rigid SfM from Casual Videos    </h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-3 text-center"><a href="https://ykasten.github.io/">Yoni Kasten</a><sup>1</sup></div>
            <div class="col-3 text-center">Wuyue Lu</a><sup>2</sup></div>
            <div class="col-3 text-center"><a href="https://haggaim.github.io/">Haggai Maron</a><sup>1,3</sup></div>
        </div>

        <div class="affil-row">
            <div class="venue text-center"><sup>1</sup>NVIDIA Research</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><sup>2</sup>Simon Fraser University</div>
        </div>
        <div class="affil-row">
            <div class="venue text-center"><sup>3</sup>Technion</div>
        </div>


        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="https://arxiv.org" target="_blank">
                <span class="material-icons">  </span> 
                 Paper
            </a>

        </div></div>
    </div>

    <section id="teaser">
            <figure style="width: 100%;">
                <video width="80%" controls="" muted="" loop="" autoplay="">
                    <source src="./files/dog3.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>

                <p class="caption" style="margin-bottom: 1px;">
                   <i> TracksTo4D</i> receives video frames with a set of pre-extracted point tracks presented in corresponding colors as inputs (left side), and maps them into dynamic 3D structures and camera positions (right side). The output camera trajectory is presented as gray frustums, whereas the current camera is marked in red. The reconstructed 3D scene points are presented in corresponding colors to the input tracks.  
                   Note that the outputs presented on this web page are obtained at inference time, with a single feed-forward prediction, without any optimization or fine-tuning, on unseen test cases.   
                </p>
            </figure>
    </section>



    <hr>
    <h1>Overview</h1>
    <hr>

    <section id="teaser-videos">
        <div class="flex-row">
       
            <div style="width: 100%;">
                <br><br>
                <p>
                   We tackle the long-standing challenge of reconstructing 3D structures and camera positions from several scene images. The problem is particularly hard when objects are transformed in a non-rigid way. Current approaches to this problem make unrealistic assumptions or require a long optimization time.  We present <i>TracksTo4D</i>,  a learning-based approach to infer 3D structure and camera positions from in-the-wild videos. Specifically, we build on recent progress in point tracking, to extract long-term point tracks from videos, and learn class-agnostic features. 

We then design a deep equivariant neural network architecture (Figure 2), that maps the point tracks of a given video into corresponding camera poses, 3D points,  and per-point non-rigid motion level values.  Training on pet videos, we observe that by just "watching" point tracks in videos, our network learns to infer their 3D structure and the camera motion. 

                </p> 

                <p>
                  Our model is trained on the <a href="https://github.com/facebookresearch/cop3d" target="_blank">Common Pets</a> dataset using only 2D point tracks extracted by <a href="https://co-tracker.github.io/" target="_blank">CoTracker</a>, without any 3D supervision by simply minimizing the reprojection errors.  We evaluate our method on test data with GT depth maps and demonstrate that it generalizes well across object categories. With a short bundle adjustment, we achieve the most accurate camera poses compared to previous methods, all while running faster. More specifically, compared to the state-of-art, we reduced the Absolute Translation Error by 18%, the Relative Translation Error 21%, and the Relative Rotation Error by 15%.  Moreover, our method produces depth accuracy comparable to the state-of-the-art method, while being ~x10 faster. We also demonstrate a certain level of generalization to entirely out-of-distribution inputs.
                </p>
                

            </div>
        </div>
    </section>

    <section id="pipeline">

        <h2>Pipeline</h2>
        <hr>

        <figure style="width: 100%;">
            <a href="./files/architecture.png">
                <img width="100%" src="./files/architecture.png">
            </a>
            <p class="caption" style="margin-bottom: 1px;">
                <b>Our pipeline.</b> Our network takes as input a set of 2D point tracks (left) and uses several multi-head attention layers while alternating between the time dimension and the track dimension (middle). The network predicts cameras, per-frame 3D points, and per-world point movement value (right). The 3D point colors illustrate the movement level values, such that points with high/low motion values are presented in blue/purple colors respectively. These outputs are used to reproject the predicted points into the frames for calculating the reprojection error losses. See details in the paper.     
            </p>
        </figure>
            </p>
        </figure>

        <h1>Results</h1>
       
        
  
        <figure style="width: 100%;">
            <video width="80%" controls="" muted="" loop="" autoplay="">
                <source src="./files/cat1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>

        </figure>
        <figure style="width: 100%;">
            <video width="80%" controls="" muted="" loop="" autoplay="">
                <source src="./files/dog2.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>

        </figure>
        <figure style="width: 100%;">
            <video width="80%" controls="" muted="" loop="" autoplay="">
                <source src="./files/dog1.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>

        </figure>
        <figure style="width: 100%;">
            <video width="80%" controls="" muted="" loop="" autoplay="">
                <source src="./files/cat2.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>

        </figure>
        



    </section>
    
    <section id="bibtex">
        <h2>Citation</h2>
        <pre><code>

        </code></pre>
    </section>

    

<br>


</div>


</body></html>
